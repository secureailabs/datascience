{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY CODE\n",
    "\n",
    "# Until smart broker is installable\n",
    "import sys\n",
    "sys.path.append(\"../../fastapi\")\n",
    "\n",
    "# Until we start using sessions properly\n",
    "class Session():\n",
    "    def __init__(self, ip, port):\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "\n",
    "session = Session(\"127.0.0.1\", \"8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAIL Researcher User Documentation\n",
    "\n",
    "**Aim:** This notebook section acts as a primer for new users on the SAIL platform.  This contains all necessary background information for working with SAIL data federations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Dataset Types in the SAIL Ecosystem\n",
    "\n",
    "There are four main dataset types which SAIL allows computation of:\n",
    "\n",
    "- Longitudinal Dataasets\n",
    "    - Longitudinal-time-Series\n",
    "    - Longitudinal-Repeated-Measurement\n",
    "    - Longitudinal-Events\n",
    "- Survey/ Cross Sectional Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Longitudinal Datasets\n",
    "\n",
    "Longitudinal Datasets represent the raw data format which is ingested into the sail platform. Longitudinal Data are not queryable in their raw form and need to first be flattened into conventional Tabular data in order to be analysed. For our purposes, Longitudinal Datasets have three types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Longitudinal-repeated-measurement\n",
    "\n",
    "This type of dataset contains the same set of measurements for different instances or subjects at different points in time. Although different amounts of measurements can be taken per subject, the set of measurements are always the same.\n",
    "\n",
    "| Instance      | Blood Pressure | Heartbeat     |\n",
    "| :---        |    :----:   |          :---: |\n",
    "| Adam Monday      | 120       | 70   |\n",
    "| Adam Tuesday   | 90        | 40      |\n",
    "| Adam Wednesday      | 80       | 60   |\n",
    "| Adam Thursday   | 72        | 42      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Longitudinal Time Series\n",
    "\n",
    "Time series data has a single repeated measurement over time in a series. Series' are specific to a subject and a type of measurement. Not all series need to be of the same length and not all data needs to follow the same timestamps.\n",
    "\n",
    "| Instance      | Monday | Tuesday     |\n",
    "| :---        |    :----:   |          :---: |\n",
    "| Anne Heartbeat  | 60       | 70   |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "| Instance      | Monday | Tuesday     |\n",
    "| :---        |    :----:   |          :---: |\n",
    "| Anne blood pressure  | 90       | 80   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Longitudinal Events\n",
    "\n",
    "This type of dataset contains combinations of times, values and types of test. The type of the value field may vary. Outside of the SAIL ecosystem this type of data is also referred to as journal data. Since missing events are simply omitted, this type of data normally does not contain missing fields.\n",
    "\n",
    "| Patient       | Value     | Test                 |   Day       |\n",
    "| :---          |    :----: |  :---:               |        :---: |\n",
    "| Adam          | 120       | Blood Pressure       | Monday      |\n",
    "| Adam          | 90        | Heartbeat            | Tuesday     |\n",
    "| Saurabh       | 80        | Blood Pressure       | Monday      |\n",
    "| Saurabh       | 85        | Fasting Blood Glucose| Wednesday   |\n",
    "| Stanley       | 72        | Blood Pressure       | Friday      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Survey/ Cross Sectional Datasets\n",
    "\n",
    "These are the most common type of dataset where there is a list of instances and a list of measurements and each measurement appears at most once for every instance. in conventional machine learning and data anlaytics this is the format in which data must exist. All longitudinal data must first be processed into this format before it can be analysed by SAIL SAFE functions.\n",
    "\n",
    "\n",
    "| Instance      | Blood Pressure  | Heartbeat            |   Fasting Blood Glucose       |\n",
    "| :---          |    :----:       |  :---:               |        :---:                  |\n",
    "| Adam          | 120             | 90                   | 101                           |\n",
    "| Anne          | 90              | 50                   | 120                           |\n",
    "| Jaap          | 80              | 70                   | 115                           |\n",
    "| Saurabh       | 85              | 80                   | 118                           |\n",
    "| Stanley       | 72              | 65                   | 110                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 SAIL Data Structures\n",
    "\n",
    "**Aim:** This section provides information on how the different datastructures described above are organised in the SAIL platform. We will detail how to process a data federation into a useable format once provisioned. Specifically, we will be looking at how to distil a *longitudinal* data federation into a *tabular* data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SAIL Data Federation Structure\n",
    "\n",
    "Federal Data relies on two main architectural components; a **dataset** and a corresponding **data model**.\n",
    "\n",
    "- Datasets contain the raw data which Researchers work with\n",
    "- Data models contain meta-information relating to the structure of Datasets\n",
    "\n",
    "###### **TODO: INSERT NICE INFOGRAPHIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SAIL Data Federation Hierarchy\n",
    "\n",
    "Federated Data adheres to the following Object types:\n",
    "\n",
    "- Longitudinal Dataset\n",
    "- Tabular Dataset\n",
    "- Dataframe\n",
    "- Series\n",
    "\n",
    "\n",
    "\\\\\n",
    "<img src=\"images/dataset_lifecycle.png\" alt=\"Data Federation Hierarchy\" width=\"800vw\"/>\n",
    "\n",
    "\n",
    "###### **TODO: INSERT NICER INFOGRAPHIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Longitudinal Dataset Structure\n",
    "\n",
    "A *Longitudinal Dataset* is a collection of *Patients* and a *Longitudinal Data Model*.\n",
    "- A Longitudinal Data Model is the FHIR profile which defines how the longitudinal dataset is structured\n",
    "- A Patient owns a list of *Measurements* and some basic information concerning the data subject. \n",
    "- A Measurement is a timepoint in patient history where a single data point was measured or a single fact was established.\n",
    "\n",
    "###### **TODO: INSERT NICE INFOGRAPHIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Tabular Dataset Structure\n",
    "\n",
    "A *Tabular Dataset* is a composition of *Data Frames* and a *Tabular Data Model*. A Tabular Dataset has its own id that is unique within the platform. Tabular Datasets can also be saved to disk to establish a breakpoint. A Tabular Data Model Contains a composition of Datframe Data Models.\n",
    "\n",
    "###### **TODO: INSERT NICE INFOGRAPHIC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Dataframe Structure\n",
    "\n",
    "A *Data Frame* is a composition of *Series* and a *Data Frame Model*. A Data Frame Model is a composition of Series Models. Most machine learning operations are performed at the Data Frame level. \n",
    "\n",
    "###### **TODO: INSERT NICE INFOGRAPHIC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Series Structure\n",
    "\n",
    "A *Series* is a composition of *Values* and a *Series Model*. A Data Frame model is a composition of Series Models. Most statistical operations use Series as input for their processing. There are two types of Series with two types of Series Model:\n",
    "\n",
    "An Interval Series Model defines:\n",
    "\n",
    "- value resolution\n",
    "- min values\n",
    "- max value\n",
    "\n",
    "A Categorical Series Model defines:\n",
    "\n",
    "- A list of categories that can be present in the series\n",
    "\n",
    "###### **TODO: INSERT NICE INFOGRAPHIC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Converting a Longitudinal dataset to a Tabular Dataset\n",
    "\n",
    "A *Longitudinal Dataset* may be converted to a *Tabular Dataset* using *Anchors* and *Aggregators*.\n",
    "\n",
    "#### 3.3.1 Anchors\n",
    "\n",
    "An Anchor is a function that establishes a time point in a patients history in a Longitudinal Dataset. Some simple default anchors would be date of birth or death. Some more complicated once could be time of initial diagnoses or the start of the 3rd round of treatment.\n",
    "\n",
    "#### 3.3.2 Aggregator\n",
    "\n",
    "An Aggregator is a function (or Visitor pattern) that can be inserted into a Longitudinal Dataset to produce a Series with one entry for each patient visited. An Aggregator is date-time agnostic but not sequence agnostic. It can select the first Measurement but not the first after, for example, January 21st 2022. Aggregators cannot refer to each other but they can refer to Anchors. For example, an Aggregator could collect the mean of the systolic blood pressure (measurement) between the end of the second round of chemo treatment (anchor) and the death of the patient (anchor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with SAIL Data\n",
    "\n",
    "**Aim:** Previously we looked at the conceptual framework for working with data in the SAIL ecosystem. In the following sections, we'll look at how that conceptual framework is applied in code. Specifically, we will process a longitudinal dataset to the point where we can perform statistical functions on Series data which has been pulled from the longitudinal dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Making Data Available\n",
    "\n",
    "**Aim:** This section will detail how to search for available data federations and provision Secure Computation Nodes (SCNs) to hold each dataset in the federation.\n",
    "\n",
    "#### 3.1.1 Finding a Data Federation\n",
    "\n",
    "##### TODO\n",
    "\n",
    "#### 3.1.2 Provisoning a Data Federation\n",
    "\n",
    "##### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Longitudinal Operations\n",
    "\n",
    "**Aim:** In this section we will look at how to both read in a longitudinal dataset and filter this into a tabular dataset.\n",
    "\n",
    "#### 3.2.1 Reading a Longitudinal Dataset\n",
    "\n",
    "In the previous sections we found the data we were looking for anf provisioned SCNs holding that longitudinal data. Now we'd like to gain a reference which we can use to refer to the federated dataset held remotely. The functionality we require in the first step is contained in the <code>data_api</code> component of our orchestrator client.\n",
    "\n",
    "We run the <code>read_longitudinal_fhirv1</code> function to gain a reference to that federated longitudinal dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/venv38_sail/lib/python3.8/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39;49mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m data_api\n\u001b[0;32m----> 3\u001b[0m longitudinal_dataset_id \u001b[39m=\u001b[39m data_api\u001b[39m.\u001b[39;49mread_longitudinal_fhirv1(session)\n",
      "File \u001b[0;32m~/datascience/notebooks/documentation/../../fastapi/smart_broker_api/data_api.py:11\u001b[0m, in \u001b[0;36mread_longitudinal_fhirv1\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_longitudinal_fhirv1\u001b[39m(session):\n\u001b[1;32m      7\u001b[0m     result \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m      8\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m session\u001b[39m.\u001b[39mip \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m session\u001b[39m.\u001b[39mport \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/ingestion/read_longitudinal/fhirv1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m         params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39;49mjson()[\u001b[39m\"\u001b[39m\u001b[39mlongitudinal_id\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/venv38_sail/lib/python3.8/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from smart_broker_api import data_api\n",
    "\n",
    "longitudinal_dataset_id = data_api.read_longitudinal_fhirv1(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a remote, federated dataset which we can refer to with the <code>id</code> returned by the <code>read_longitudinal_fhirv1</code> function. However, longitudinal datasets as outlined above are not suitable to be operated on by machine learning or statistical functions. First we must distill this raw format into either survey/ cross sectional data or series. In the next section we will define the data models required to manage this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Building Our Desired Tabular Data Structure\n",
    "\n",
    "In order to convert a longitudinal dataset to a workable format we must process our desired features into a tabular tabular form. The first step is to define a data model which refers to our tabular data. As tabular data models are a composition of data frame models which are in turn compositions of series models, we must define this from the series level up.\n",
    "\n",
    "We perform this using functionality contained in the <code>data_model_api</code> component of our orchestrator client. In the code block below, first we define a <code>data_frame</code> model to be populated by a set of series models. We do this with <code>data_model_api.create_date_frame</code>. We pass the desired name for our data frame as we create the data frame model. We recieve a reference to the newly minted data frame model which we can use to work with it remotely.\n",
    "\n",
    "###### TODO: Rationalise how we build up data models. We should begin at the smallest components and work our way larger or we should begin at the largest component and populate it with progressively smaller components. Currently we begin in the middle with Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smart_broker_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test copy.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m data_model_api\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data_model_id \u001b[39m=\u001b[39m data_model_api\u001b[39m.\u001b[39mcreate_date_frame(session, \u001b[39m\"\u001b[39m\u001b[39mdata_frame_0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data_model_api\u001b[39m.\u001b[39mdata_frame_add_series(session, data_model_id, \u001b[39m\"\u001b[39m\u001b[39mbmi_mean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mObservation:Body Mass Index\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAgregatorIntervalMean\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smart_broker_api'"
     ]
    }
   ],
   "source": [
    "from smart_broker_api import data_model_api\n",
    "\n",
    "data_frame_model_id = data_model_api.create_date_frame(session, \"data_frame_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a <code>data_frame_model</code> to populate, we can define the individual <code>series</code> which will fit into this model. For the purpose of this demo, we will allocate three new <code>series</code> models to the <code>data_frame_model</code>. We do this with <code>data_model_api.data_frame_add_series</code> from the data_model component of the orchestrator client.\n",
    "\n",
    "<code>data_frame_add_series</code> takes the following parameters:\n",
    "\n",
    "- The <code>id</code> of the <code>data_frame_model</code> we are adding these series to\n",
    "- The <code>name</code> of the series series we are specifying\n",
    "- The <code>observation</code> to be pulled from the longitudinal dataset\n",
    "- The <code>aggregator</code> which will be used to pull the <code>observation</code> from the longitudinal dataset\n",
    "\n",
    "Below we add three new series models to our dataframe model, which we may refer to remotely with <code>data_frame_model_id</code>.\n",
    "\n",
    "###### TODO: Related to the todo above. We should be able to define series models as atomic compoinents which may stand alone, outside of a dataframe model. They may then be added to a dataframe model in the same way that a dataframe model is added to a tabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_api.data_frame_add_series(session, data_frame_model_id, \"bmi_mean\", \"Observation:Body Mass Index\", \"AgregatorIntervalMean\")\n",
    "data_model_api.data_frame_add_series(session, data_frame_model_id, \"bmi_first\", \"Observation:Body Mass Index\", \"AgregatorIntervalFirstOccurance\")\n",
    "data_model_api.data_frame_add_series(session, data_frame_model_id, \"bmi_last\", \"Observation:Body Mass Index\", \"AgregatorIntervalLastOccurance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to generate our tabular data model. We create an empty tabular data model with <code>create_tabular_data</code> from the <code>data_model_api</code> component of our orchestrator client. We recieve a reference to the new data model, <code>data_model_tabular_id</code>, which we can use to work with it remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_tabular_id = data_model_api.create_tabular_data(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may then add the previously defined dataframe model to the empty tabular data model to create a complete set of meta-information relating to the tabular data we'd like to pull from our longitudinal dataset. THe functionality we use to achieve this is held in the <code> data_model_api </code> component of the orchestrator client. The specific function is <code>tabular_add_dataframe</code>. <code>tabular_add_dataframe</code> takes the following parameters:\n",
    "\n",
    "- the <code>id</code> of the dataframe to be added to the tabular data model\n",
    "- the <code>id</code> of the tabular data model which the dataframe will be added to\n",
    "\n",
    "Once built, we can refer to this entire structure with <code>data_model_tabular_id</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_tabular_id = data_model_api.tabular_add_dataframe(session, data_frame_model_id, data_model_tabular_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we complete this section, we now have a fully defined data model which contains the following structure:\n",
    "\n",
    "- Tabular Data model\n",
    "    - Data Frame Model ('data_frame_0')\n",
    "        - Series Model ('bmi_mean')\n",
    "        - Series Model ('bmi_first')\n",
    "        - Series Model ('bmi_last)\n",
    "\n",
    "We are now ready to parse the values specifed in this data model from our original longitudinal dataset.\n",
    "\n",
    "###### TODO: Replace ugly list with pretty diagram of defined data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Parse Tabular Dataset from Longitudinal Dataset Model Specification\n",
    "\n",
    "In this section we'll be using our newly defined tabular data model to parse from our longitudinal dataset. We'll end up with a tabular dataset which we can perform analytics on downstream. We'll be using the <code>data_api</code> component of the orchestrator to achieve this. Specifically, we'll be using <code>parse_dataset_tabular_from_longitudinal</code>. This takes the following parameters:\n",
    "\n",
    "- The <code> id </code> of the longitudinal dataset we are parsing from\n",
    "- The <code> id </code> of the tabular data model we defined above\n",
    "- dataset_federation_id *(It's not clear this is a necessary parameter)*\n",
    "- dataset_federation_name *(It's not clear this a necessary parameter)*\n",
    "\n",
    "When this has run to completion we will recieve a reference to our newly minted tabular dataset.\n",
    "\n",
    "###### TODO: Why are we using dataset_federation_id and dataset_federation_name if we already have access to the longitudinal data? ADD TO TECHDEBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smart_broker_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test copy.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m data_api\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dataset_federation_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma892f738-4f6f-11ed-bdc3-0242ac120002\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset_federation_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr4sep2019_csvv1_20_1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smart_broker_api'"
     ]
    }
   ],
   "source": [
    "from smart_broker_api import data_api\n",
    "\n",
    "dataset_federation_id = \"a892f738-4f6f-11ed-bdc3-0242ac120002\"\n",
    "dataset_federation_name = \"r4sep2019_csvv1_20_1\"\n",
    "\n",
    "tabular_dataset_id = data_api.parse_dataset_tabular_from_longitudinal(session, longitudinal_dataset_id, dataset_federation_id, dataset_federation_name, data_model_tabular_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tabular Dataset Operations\n",
    "\n",
    "In the previous sections we provisioned a longitudinal dataset, defined a tabular data model and used this data model to parse a tabular dataset from the longitudinal dataset. We will now select an individual data frame from our tabular datset which will be consumable by machine learning and pre-processing functionality.\n",
    "\n",
    "#### 3.3.1 Selecting a Data Frame from a Tabular Dataset\n",
    "\n",
    "in order to select a specifc data frame from our tabular dataset, we will use <code>data_frame_tabular_select_data_frame</code> from the <code>data_api</code> component of our orchestrator client. This function takes the following parameters:\n",
    "\n",
    "- The <code>id</code> of the tabular dataset we will be selecting from\n",
    "- The <code>name</code> of the particular data frame we would like to process\n",
    "\n",
    "We will recieve the <code>id</code> of the data frame specified by the data frame <code>name</code> provided as a parameter. This will allow us to refer to that particular data frame remotely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_id = data_api.data_frame_tabular_select_data_frame(session, tabular_dataset_id, \"data_frame_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Frame Level Operations\n",
    "\n",
    "In the previous section we selected an individual data frame from the tabular dataset we created earlier. In this section we will learn how to perform operations on that data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Removing Missing values from a Data Frame\n",
    "\n",
    "Now we have the reference to an individual dataframe, we can perform preprocessing operations on said data. In this case we will be removing all missing values from the data frame selected in the previous section. To do this, we employ the <code>preprocessing</code> component of the orchestrator client. The specific function from this component that we'll be using is <code>drop_na_data_frame</code> which takes the <code>id</code> of the original data frame and returns the <code>id</code> of the new dataframe with all missing values removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smart_broker_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test copy.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing_api\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m no_nan_data_frame_id \u001b[39m=\u001b[39m preprocessing_api\u001b[39m.\u001b[39mdrop_na_data_frame(session, data_frame_id)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smart_broker_api'"
     ]
    }
   ],
   "source": [
    "from smart_broker_api import preprocessing_api\n",
    "\n",
    "no_na_data_frame_id = preprocessing_api.drop_na_data_frame(session, data_frame_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Selecting a Series from a Data Frame\n",
    "\n",
    "We can select an individual series from a data frame by once again using the <code>data_api</code> component of the orchestrator client. The function used here is <code>data_frame_select_series</code>. This takes two parameters:\n",
    "\n",
    "- The <code>id</code> of the data frame beng selected from\n",
    "- The <code>name</code> of the desired series\n",
    "\n",
    "Below we select two series from our preprocessed dataframe. We recieve back the <code>id</code> of the series' which are referred to by <code>name</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_1_id = data_api.data_frame_select_series(session, no_na_data_frame_id, \"bmi_mean\")\n",
    "series_2_id = data_api.data_frame_select_series(session, no_na_data_frame_id, \"bmi_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Series Level Data Operations\n",
    "\n",
    "We now have a reference to two series which have been selected all the way from our original longitudinal dataset. We can use these series to perform some statistical analysis and data visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1 Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smart_broker_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test copy.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y126sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m statistics_api\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y126sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_broker_api\u001b[39;00m \u001b[39mimport\u001b[39;00m visualization_api\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/SAIL/datascience/fastapi/test%20copy.ipynb#Y126sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_objects\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgo\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smart_broker_api'"
     ]
    }
   ],
   "source": [
    "from smart_broker_api import statistics_api\n",
    "from smart_broker_api import visualization_api\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "type_distribution=\"normalunit\"\n",
    "type_ranking=\"cdf\"\n",
    "alternative = \"two-sided\"\n",
    "print(statistics_api.count(session,  series_1_id))\n",
    "print(statistics_api.mean(session,  series_1_id))\n",
    "print(statistics_api.chisquare(session,  series_1_id, series_2_id))\n",
    "print(statistics_api.kolmogorovSmirnovTest(session,  series_1_id, type_distribution, type_ranking))\n",
    "print(statistics_api.kurtosis(session,  series_1_id))\n",
    "print(statistics_api.levene_test(session,  series_1_id, series_2_id))\n",
    "print(statistics_api.mann_whitney_u_test(session,  series_1_id, series_2_id, alternative, type_ranking))\n",
    "print(statistics_api.min_max(session,  series_1_id))\n",
    "print(statistics_api.paired_t_test(session,  series_1_id, series_2_id, alternative))\n",
    "print(statistics_api.pearson(session,  series_1_id, series_2_id, alternative))\n",
    "print(statistics_api.skewness(session,  series_1_id))\n",
    "print(statistics_api.spearman(session,  series_1_id, series_2_id, alternative, type_ranking))\n",
    "print(statistics_api.student_t_test(session,  series_1_id, series_2_id, alternative))\n",
    "print(statistics_api.variance(session,  series_1_id))\n",
    "print(statistics_api.welch_t_test(session,  series_1_id, series_2_id, alternative))\n",
    "print(statistics_api.wilcoxon_signed_rank_test(session,  series_1_id, series_2_id, alternative, type_ranking))\n",
    "print(statistics_api.wilcoxon_signed_rank_test(session,  series_1_id, series_2_id, alternative, type_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_of_fig = visualization_api.histogram(session, series_1_id, 20)\n",
    "fig = go.Figure(dict_of_fig[\"figure\"])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "dict_of_fig = visualization_api.kernel_density_estimation(session, series_1_id, 2)\n",
    "fig = go.Figure(dict_of_fig[\"figure\"])\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv38_sail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229c0ee7a8d57afd62214254ad4dcd3448989c886ec30669907354b54cc615b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
