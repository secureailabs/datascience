{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_federation = [[x_train, y_train], [x_train, y_train]]\n",
    "\n",
    "in_layer = len(data_federation[0][0][0])\n",
    "out_layer = len(data_federation[0][1][0])\n",
    "optimizer = \"SGD\"\n",
    "criterion = \"MSELoss\"\n",
    "starting_model = torch.rand(in_layer, out_layer)\n",
    "epochs = 100\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sail_safe_functions.machine_learning.models.LinearRegression import LinearRegression\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearRegression(in_layer, out_layer)\n",
    "\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(179.3457, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 179.3457489013672\n",
      "tensor(15.0617, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 15.061685562133789\n",
      "tensor(1.6567, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 1.656721830368042\n",
      "tensor(0.5585, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 0.5585394501686096\n",
      "tensor(0.4642, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 0.4642362892627716\n",
      "tensor(0.4519, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 0.4518689215183258\n",
      "tensor(0.4462, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 0.44623681902885437\n",
      "tensor(0.4412, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 0.4412059783935547\n",
      "tensor(0.4363, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 0.43627509474754333\n",
      "tensor(0.4314, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 0.4314028024673462\n",
      "tensor(0.4266, grad_fn=<MseLossBackward0>)\n",
      "epoch 10, loss 0.42658570408821106\n",
      "tensor(0.4218, grad_fn=<MseLossBackward0>)\n",
      "epoch 11, loss 0.4218219816684723\n",
      "tensor(0.4171, grad_fn=<MseLossBackward0>)\n",
      "epoch 12, loss 0.4171117842197418\n",
      "tensor(0.4125, grad_fn=<MseLossBackward0>)\n",
      "epoch 13, loss 0.4124537706375122\n",
      "tensor(0.4078, grad_fn=<MseLossBackward0>)\n",
      "epoch 14, loss 0.40784788131713867\n",
      "tensor(0.4033, grad_fn=<MseLossBackward0>)\n",
      "epoch 15, loss 0.4032934904098511\n",
      "tensor(0.3988, grad_fn=<MseLossBackward0>)\n",
      "epoch 16, loss 0.3987899720668793\n",
      "tensor(0.3943, grad_fn=<MseLossBackward0>)\n",
      "epoch 17, loss 0.3943368196487427\n",
      "tensor(0.3899, grad_fn=<MseLossBackward0>)\n",
      "epoch 18, loss 0.38993358612060547\n",
      "tensor(0.3856, grad_fn=<MseLossBackward0>)\n",
      "epoch 19, loss 0.38557910919189453\n",
      "tensor(0.3813, grad_fn=<MseLossBackward0>)\n",
      "epoch 20, loss 0.381273478269577\n",
      "tensor(0.3770, grad_fn=<MseLossBackward0>)\n",
      "epoch 21, loss 0.377015620470047\n",
      "tensor(0.3728, grad_fn=<MseLossBackward0>)\n",
      "epoch 22, loss 0.3728055953979492\n",
      "tensor(0.3686, grad_fn=<MseLossBackward0>)\n",
      "epoch 23, loss 0.3686426281929016\n",
      "tensor(0.3645, grad_fn=<MseLossBackward0>)\n",
      "epoch 24, loss 0.3645258843898773\n",
      "tensor(0.3605, grad_fn=<MseLossBackward0>)\n",
      "epoch 25, loss 0.3604554831981659\n",
      "tensor(0.3564, grad_fn=<MseLossBackward0>)\n",
      "epoch 26, loss 0.35643041133880615\n",
      "tensor(0.3525, grad_fn=<MseLossBackward0>)\n",
      "epoch 27, loss 0.35245004296302795\n",
      "tensor(0.3485, grad_fn=<MseLossBackward0>)\n",
      "epoch 28, loss 0.34851405024528503\n",
      "tensor(0.3446, grad_fn=<MseLossBackward0>)\n",
      "epoch 29, loss 0.34462234377861023\n",
      "tensor(0.3408, grad_fn=<MseLossBackward0>)\n",
      "epoch 30, loss 0.34077414870262146\n",
      "tensor(0.3370, grad_fn=<MseLossBackward0>)\n",
      "epoch 31, loss 0.33696895837783813\n",
      "tensor(0.3332, grad_fn=<MseLossBackward0>)\n",
      "epoch 32, loss 0.33320581912994385\n",
      "tensor(0.3295, grad_fn=<MseLossBackward0>)\n",
      "epoch 33, loss 0.3294849097728729\n",
      "tensor(0.3258, grad_fn=<MseLossBackward0>)\n",
      "epoch 34, loss 0.32580551505088806\n",
      "tensor(0.3222, grad_fn=<MseLossBackward0>)\n",
      "epoch 35, loss 0.32216760516166687\n",
      "tensor(0.3186, grad_fn=<MseLossBackward0>)\n",
      "epoch 36, loss 0.31857001781463623\n",
      "tensor(0.3150, grad_fn=<MseLossBackward0>)\n",
      "epoch 37, loss 0.3150124251842499\n",
      "tensor(0.3115, grad_fn=<MseLossBackward0>)\n",
      "epoch 38, loss 0.3114946484565735\n",
      "tensor(0.3080, grad_fn=<MseLossBackward0>)\n",
      "epoch 39, loss 0.30801627039909363\n",
      "tensor(0.3046, grad_fn=<MseLossBackward0>)\n",
      "epoch 40, loss 0.30457693338394165\n",
      "tensor(0.3012, grad_fn=<MseLossBackward0>)\n",
      "epoch 41, loss 0.30117592215538025\n",
      "tensor(0.2978, grad_fn=<MseLossBackward0>)\n",
      "epoch 42, loss 0.29781243205070496\n",
      "tensor(0.2945, grad_fn=<MseLossBackward0>)\n",
      "epoch 43, loss 0.2944869101047516\n",
      "tensor(0.2912, grad_fn=<MseLossBackward0>)\n",
      "epoch 44, loss 0.29119834303855896\n",
      "tensor(0.2879, grad_fn=<MseLossBackward0>)\n",
      "epoch 45, loss 0.28794682025909424\n",
      "tensor(0.2847, grad_fn=<MseLossBackward0>)\n",
      "epoch 46, loss 0.28473103046417236\n",
      "tensor(0.2816, grad_fn=<MseLossBackward0>)\n",
      "epoch 47, loss 0.2815517783164978\n",
      "tensor(0.2784, grad_fn=<MseLossBackward0>)\n",
      "epoch 48, loss 0.278407484292984\n",
      "tensor(0.2753, grad_fn=<MseLossBackward0>)\n",
      "epoch 49, loss 0.2752985656261444\n",
      "tensor(0.2722, grad_fn=<MseLossBackward0>)\n",
      "epoch 50, loss 0.27222439646720886\n",
      "tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
      "epoch 51, loss 0.2691844403743744\n",
      "tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
      "epoch 52, loss 0.26617851853370667\n",
      "tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
      "epoch 53, loss 0.2632063925266266\n",
      "tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
      "epoch 54, loss 0.2602670192718506\n",
      "tensor(0.2574, grad_fn=<MseLossBackward0>)\n",
      "epoch 55, loss 0.2573607861995697\n",
      "tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
      "epoch 56, loss 0.25448670983314514\n",
      "tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
      "epoch 57, loss 0.25164511799812317\n",
      "tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
      "epoch 58, loss 0.24883480370044708\n",
      "tensor(0.2461, grad_fn=<MseLossBackward0>)\n",
      "epoch 59, loss 0.2460564523935318\n",
      "tensor(0.2433, grad_fn=<MseLossBackward0>)\n",
      "epoch 60, loss 0.2433086484670639\n",
      "tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
      "epoch 61, loss 0.24059142172336578\n",
      "tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
      "epoch 62, loss 0.2379048764705658\n",
      "tensor(0.2352, grad_fn=<MseLossBackward0>)\n",
      "epoch 63, loss 0.2352481335401535\n",
      "tensor(0.2326, grad_fn=<MseLossBackward0>)\n",
      "epoch 64, loss 0.23262125253677368\n",
      "tensor(0.2300, grad_fn=<MseLossBackward0>)\n",
      "epoch 65, loss 0.23002353310585022\n",
      "tensor(0.2275, grad_fn=<MseLossBackward0>)\n",
      "epoch 66, loss 0.22745494544506073\n",
      "tensor(0.2249, grad_fn=<MseLossBackward0>)\n",
      "epoch 67, loss 0.22491507232189178\n",
      "tensor(0.2224, grad_fn=<MseLossBackward0>)\n",
      "epoch 68, loss 0.22240354120731354\n",
      "tensor(0.2199, grad_fn=<MseLossBackward0>)\n",
      "epoch 69, loss 0.2199198305606842\n",
      "tensor(0.2175, grad_fn=<MseLossBackward0>)\n",
      "epoch 70, loss 0.21746422350406647\n",
      "tensor(0.2150, grad_fn=<MseLossBackward0>)\n",
      "epoch 71, loss 0.21503567695617676\n",
      "tensor(0.2126, grad_fn=<MseLossBackward0>)\n",
      "epoch 72, loss 0.21263442933559418\n",
      "tensor(0.2103, grad_fn=<MseLossBackward0>)\n",
      "epoch 73, loss 0.21026001870632172\n",
      "tensor(0.2079, grad_fn=<MseLossBackward0>)\n",
      "epoch 74, loss 0.2079121470451355\n",
      "tensor(0.2056, grad_fn=<MseLossBackward0>)\n",
      "epoch 75, loss 0.20559029281139374\n",
      "tensor(0.2033, grad_fn=<MseLossBackward0>)\n",
      "epoch 76, loss 0.2032945156097412\n",
      "tensor(0.2010, grad_fn=<MseLossBackward0>)\n",
      "epoch 77, loss 0.20102433860301971\n",
      "tensor(0.1988, grad_fn=<MseLossBackward0>)\n",
      "epoch 78, loss 0.19877958297729492\n",
      "tensor(0.1966, grad_fn=<MseLossBackward0>)\n",
      "epoch 79, loss 0.19655968248844147\n",
      "tensor(0.1944, grad_fn=<MseLossBackward0>)\n",
      "epoch 80, loss 0.19436492025852203\n",
      "tensor(0.1922, grad_fn=<MseLossBackward0>)\n",
      "epoch 81, loss 0.19219455122947693\n",
      "tensor(0.1900, grad_fn=<MseLossBackward0>)\n",
      "epoch 82, loss 0.19004839658737183\n",
      "tensor(0.1879, grad_fn=<MseLossBackward0>)\n",
      "epoch 83, loss 0.1879260540008545\n",
      "tensor(0.1858, grad_fn=<MseLossBackward0>)\n",
      "epoch 84, loss 0.18582747876644135\n",
      "tensor(0.1838, grad_fn=<MseLossBackward0>)\n",
      "epoch 85, loss 0.18375219404697418\n",
      "tensor(0.1817, grad_fn=<MseLossBackward0>)\n",
      "epoch 86, loss 0.18170036375522614\n",
      "tensor(0.1797, grad_fn=<MseLossBackward0>)\n",
      "epoch 87, loss 0.17967139184474945\n",
      "tensor(0.1777, grad_fn=<MseLossBackward0>)\n",
      "epoch 88, loss 0.17766498029232025\n",
      "tensor(0.1757, grad_fn=<MseLossBackward0>)\n",
      "epoch 89, loss 0.1756809800863266\n",
      "tensor(0.1737, grad_fn=<MseLossBackward0>)\n",
      "epoch 90, loss 0.17371925711631775\n",
      "tensor(0.1718, grad_fn=<MseLossBackward0>)\n",
      "epoch 91, loss 0.17177943885326385\n",
      "tensor(0.1699, grad_fn=<MseLossBackward0>)\n",
      "epoch 92, loss 0.1698611080646515\n",
      "tensor(0.1680, grad_fn=<MseLossBackward0>)\n",
      "epoch 93, loss 0.1679641455411911\n",
      "tensor(0.1661, grad_fn=<MseLossBackward0>)\n",
      "epoch 94, loss 0.16608871519565582\n",
      "tensor(0.1642, grad_fn=<MseLossBackward0>)\n",
      "epoch 95, loss 0.16423407196998596\n",
      "tensor(0.1624, grad_fn=<MseLossBackward0>)\n",
      "epoch 96, loss 0.16239987313747406\n",
      "tensor(0.1606, grad_fn=<MseLossBackward0>)\n",
      "epoch 97, loss 0.160586416721344\n",
      "tensor(0.1588, grad_fn=<MseLossBackward0>)\n",
      "epoch 98, loss 0.15879328548908234\n",
      "tensor(0.1570, grad_fn=<MseLossBackward0>)\n",
      "epoch 99, loss 0.15702007710933685\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    inputs = Variable(x_train)\n",
    "    labels = Variable(y_train)\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26288065]\n",
      " [ 2.3690324 ]\n",
      " [ 4.4751844 ]\n",
      " [ 6.5813365 ]\n",
      " [ 8.687488  ]\n",
      " [10.793639  ]\n",
      " [12.899792  ]\n",
      " [15.005943  ]\n",
      " [17.112095  ]\n",
      " [19.218246  ]\n",
      " [21.324398  ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predicted = model(x_train).data.numpy()\n",
    "print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
