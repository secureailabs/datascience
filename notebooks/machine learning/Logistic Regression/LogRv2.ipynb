{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def print_graph(predicted, x_train, y_train):\n",
                "\n",
                "\n",
                "    predictions = []\n",
                "    for prediction in predicted:\n",
                "            predictions.append(prediction.argmax())\n",
                "\n",
                "    labels = []\n",
                "    for y in y_train:\n",
                "            labels.append(y.argmax())\n",
                "\n",
                "    column_a = []\n",
                "    for x in x_train:\n",
                "        column_a.append(x[1])\n",
                "\n",
                "    plt.plot(column_a, labels, 'go', label='True data', alpha=0.2)\n",
                "    plt.plot(column_a, predictions, 'go', label='Predictions', color=\"red\", alpha=0.2)\n",
                "    plt.legend(loc='best')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set up dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy\n",
                "\n",
                "def unison_shuffled_copies(a, b):\n",
                "    assert len(a) == len(b)\n",
                "    numpy.random.seed (1)\n",
                "    p = numpy.random.permutation(len(a))\n",
                "    return a[p], b[p]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_test_federation_split(df1, df2):\n",
                "\n",
                "    train = df1.sample(frac=0.8,random_state=0)\n",
                "    test = df1.drop(train.index)\n",
                "    \n",
                "    train = df2.sample(frac=0.8,random_state=0)\n",
                "    test = df2.drop(train.index)\n",
                "\n",
                "    train1 = train.sample(frac=0.5,random_state=0)\n",
                "    train2 = train.drop(train.index)\n",
                "\n",
                "    return train1, train2, test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from sklearn.datasets import load_iris\n",
                "# from sklearn.preprocessing import OneHotEncoder\n",
                "# import pandas as pd \n",
                "# import torch \n",
                "\n",
                "# # Bring in model from shared helper lib\n",
                "# from helper_libs.shared.models.LogisticRegression import LogisticRegression\n",
                "\n",
                "# iris = load_iris()\n",
                "\n",
                "# x = iris.data\n",
                "# encoder = OneHotEncoder(sparse=False)\n",
                "# y =encoder.fit_transform(iris.target.reshape(-1, 1))\n",
                "\n",
                "# x = torch.from_numpy(x)\n",
                "# y = torch.from_numpy(y)\n",
                "\n",
                "# x, y = unison_shuffled_copies(x, y)\n",
                "\n",
                "# X_df = pd.DataFrame(x)\n",
                "# Y_df = pd.DataFrame(y)\n",
                "# #\n",
                "# # train_1, train_2, test = get_test_federation_split(X_df, Y_df)\n",
                "\n",
                "# data_federation = [[X_df, Y_df], [X_df, Y_df]]\n",
                "\n",
                "# in_layer = len(X_df.columns)\n",
                "# out_layer = len(Y_df.columns)\n",
                "# optimizer = \"SGD\"\n",
                "# criterion = \"MSELoss\"\n",
                "# starting_model = LogisticRegression(in_layer, out_layer)\n",
                "# learn_rate = 0.1\n",
                "# epochs=5000\n",
                "# federal_epochs=2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# type(data_federation[0][0].values)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.Tensor([data_federation[0][0].values])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'load_iris' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic Regression/LogRv2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# Bring in model from shared helper lib\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhelper_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshared\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLogisticRegression\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m iris \u001b[39m=\u001b[39m load_iris()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m x_train \u001b[39m=\u001b[39m iris\u001b[39m.\u001b[39mdata\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000007vscode-remote?line=11'>12</a>\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'load_iris' is not defined"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "# Bring in model from shared helper lib\n",
                "from helper_libs.shared.models.LogisticRegression import LogisticRegression\n",
                "\n",
                "\n",
                "iris = load_iris()\n",
                "\n",
                "\n",
                "x_train = iris.data\n",
                "encoder = OneHotEncoder(sparse=False)\n",
                "y_train =encoder.fit_transform(iris.target.reshape(-1, 1))\n",
                "\n",
                "x_train = torch.from_numpy(x_train)\n",
                "y_train = torch.from_numpy(y_train)\n",
                "x_train = x_train.float()\n",
                "y_train = y_train.float()\n",
                "\n",
                "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
                "\n",
                "data_federation = [[x_train[0:64], y_train[0:64]], [x_train[65:129], y_train[65:129]]]\n",
                "\n",
                "x_test = x_train[130:149]\n",
                "y_test = y_train[130:149]\n",
                "\n",
                "in_layer = len(x_train[0])\n",
                "out_layer = len(y_train[0])\n",
                "optimizer = \"SGD\"\n",
                "criterion = \"MSELoss\"\n",
                "starting_model = LogisticRegression(in_layer, out_layer)\n",
                "learn_rate = 0.1\n",
                "epochs=5000\n",
                "federal_epochs=2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "new(): data must be a sequence (got builtin_function_or_method)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic Regression/LogRv2.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msail_safe_functions_orchestrator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmachine_learning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfederated_averaging\u001b[39;00m \u001b[39mimport\u001b[39;00m federated_averaging\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m federated_averaging(epochs, federal_epochs, data_federation, learn_rate, starting_model, criterion, optimizer)\n",
                        "File \u001b[0;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py:27\u001b[0m, in \u001b[0;36mfederated_averaging\u001b[0;34m(epochs, federal_epochs, data_federation, learn_rate, starting_model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=23'>24</a>\u001b[0m trained_models \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data_federation)):\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=25'>26</a>\u001b[0m     trained_models\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=26'>27</a>\u001b[0m         ModelTrain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=27'>28</a>\u001b[0m             epochs,\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=28'>29</a>\u001b[0m             data_federation[j],\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=29'>30</a>\u001b[0m             learn_rate,\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=30'>31</a>\u001b[0m             avg_model,\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=31'>32</a>\u001b[0m             criterion,\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=32'>33</a>\u001b[0m             optimizer,\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=33'>34</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=34'>35</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=35'>36</a>\u001b[0m avg_model \u001b[39m=\u001b[39m ModelAverage\u001b[39m.\u001b[39mrun(trained_models)\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions-orchestrator/sail_safe_functions_orchestrator/machine_learning/federated_averaging.py?line=37'>38</a>\u001b[0m avg_model \u001b[39m=\u001b[39m ModelRetrieve\u001b[39m.\u001b[39mrun(avg_model)\n",
                        "File \u001b[0;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py:34\u001b[0m, in \u001b[0;36mModelTrain.run\u001b[0;34m(epochs, data, learn_rate, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=26'>27</a>\u001b[0m criterion = ModelTrain.get_criterion(criterion)\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=27'>28</a>\u001b[0m optimizer = ModelTrain.get_optimizer(model, learn_rate, optimizer)\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=29'>30</a>\u001b[0m # Transform DataFrames to Tensors\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=30'>31</a>\u001b[0m # inputs = data[0].values\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=31'>32</a>\u001b[0m # labels = data[1].values  #\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=32'>33</a>\u001b[0m \n\u001b[0;32m---> <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=33'>34</a>\u001b[0m # inputs = torch.Tensor(inputs)\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=34'>35</a>\u001b[0m # labels = torch.Tensor(labels)\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=36'>37</a>\u001b[0m for epoch in range(epochs):\n\u001b[1;32m     <a href='file:///mnt/c/Users/AdamHall/Documents/GitHub/datascience/sail-safe-functions/sail_safe_functions/machine_learning/ModelTrain.py?line=38'>39</a>\u001b[0m     inputs = Variable(data[0])\n",
                        "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got builtin_function_or_method)"
                    ]
                }
            ],
            "source": [
                "from sail_safe_functions.aggregator.machine_learning.federated_averaging import federated_averaging\n",
                "\n",
                "model = federated_averaging(epochs, federal_epochs, data_federation, learn_rate, starting_model, criterion, optimizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model Before and After"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'x_test' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic Regression/LogRv2.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m predicted \u001b[39m=\u001b[39m starting_model(x_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m print_graph(predicted, x_test, y_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/Logistic%20Regression/LogRv2.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m predicted \u001b[39m=\u001b[39m model(x_test)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
                    ]
                }
            ],
            "source": [
                "predicted = starting_model(x_test)\n",
                "print_graph(predicted, x_test, y_test)\n",
                "\n",
                "predicted = model(x_test)\n",
                "print_graph(predicted, x_test, y_test)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}