{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LinearRegressionClient:\n",
    "    \n",
    "    class LinearRegression(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, in_layer, out_layer, criterion, optimizer):\n",
    "            super(LinearRegression, self).__init__()\n",
    "            self.linear = torch.nn.Linear(in_layer, out_layer, bias=False)\n",
    "            self.criterion = criterion\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        def forward(self, x):\n",
    "            y_pred = self.linear(x)\n",
    "            return y_pred\n",
    "\n",
    "        def train_model(self, epochs, x_data, y_data):\n",
    "            for epoch in range(epochs):\n",
    "                #Set model ready to train\n",
    "                self.linear.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                y_pred = self.linear(x_data)\n",
    "                # Compute Loss\n",
    "                loss = self.criterion(y_pred, y_data)\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(\"Loss: \"+ str(loss.data.view(-1)))\n",
    "\n",
    "    \n",
    "        def set_parameters_from_tensor(self, weights: torch.Tensor):\n",
    "            if not isinstance(weights, torch.Tensor):\n",
    "                print(\"Converting weights to tensor type\")\n",
    "                weights = torch.tensor(weights)\n",
    "            current_index = 0 \n",
    "            for parameter in self.parameters():\n",
    "                numel = parameter.data.numel()\n",
    "                size = parameter.data.size()\n",
    "                parameter.data.copy_(\n",
    "                                weights[current_index:current_index +\n",
    "                                                    numel].view(size))\n",
    "                current_index+=1\n",
    "\n",
    "        def get_parameters_as_tensor(self):\n",
    "            params = [param.data.view(-1) for param in self.parameters()]\n",
    "            params = torch.cat(params)\n",
    "            params = params.cpu()\n",
    "            return params\n",
    "\n",
    "    def run(in_layer, out_layer, epochs, data, learn_rate=0.1, criterion=None, optimizer=None, set_parameters=False, incoming_parameters=None):\n",
    "        criterion = torch.nn.MSELoss(size_average=False)\n",
    "        optimizer = torch.optim.SGD(self.linear.parameters(), lr=learn_rate)\n",
    "\n",
    "        model = LinearRegression(in_layer, out_layer, criterion, optimizer)\n",
    "\n",
    "        if set_parameters:\n",
    "            model.set_parameters_as_tensor(incoming_parameters)\n",
    "        \n",
    "        model.train_model(epochs, data[0], data[1])\n",
    "\n",
    "        return model.get_parameters_as_tensor()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionFederated:\n",
    "\n",
    "    def get_average_model(self, model_parameters):\n",
    "        \n",
    "        #TODO: Push list of model parameters to some SCN\n",
    "        for param in model_parameters:\n",
    "            break\n",
    "            # param.send(models[random])\n",
    "\n",
    "        #TODO: Calculate the Mean Remotely using existing safe object\n",
    "        model_sum = 0.\n",
    "        for param in model_parameters:\n",
    "            model_sum += param\n",
    "        model_param_avg = model_sum/len(model_parameters)\n",
    "\n",
    "        #TODO: Pull result to orchestrator\n",
    "        return model_param_avg\n",
    "    \n",
    "    def train_cycle(self, in_layer, out_layer, epochs, first_cycle=True, avg_parameters=None):\n",
    "        #train each model fresh\n",
    "        if first_cycle:\n",
    "            avg_parameters = []\n",
    "            for i in range(len(self.data_federation)):\n",
    "                models.append(LinearRegressionClient.run(in_layer, out_layer, epochs, self.data_federation[i], None, None, False, None))\n",
    "        #train each model based on previous average\n",
    "        else:\n",
    "            for i in range(len(self.data_federation)):\n",
    "                models.append(LinearRegressionClient.run(in_layer, out_layer, epochs, self.data_federation[i], None, None, True, avg_model))\n",
    "\n",
    "\n",
    "        #average the models and return\n",
    "        return self.get_average_model(models)\n",
    "\n",
    "    def run(data_federation, in_layer, out_layer, epochs, batches,):\n",
    "                \n",
    "        self.data_federation = data_federation\n",
    "        avg_model = train_cycle(in_layer, out_layer, epochs)\n",
    "        for batch in range(batches-1):\n",
    "            train_cycle(in_layer, out_layer, epochs, True, avg_model)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'criterion' and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb#ch0000002vscode-remote?line=4'>5</a>\u001b[0m y_dataB \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mTensor([[\u001b[39m60.0\u001b[39m], [\u001b[39m70.0\u001b[39m], [\u001b[39m40.0\u001b[39m], [\u001b[39m20.0\u001b[39m]]))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m data_federation \u001b[39m=\u001b[39m [[x_dataA, y_dataA], [x_dataB, y_dataB]]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb#ch0000002vscode-remote?line=7'>8</a>\u001b[0m modelA \u001b[39m=\u001b[39m LinearRegressionClient(\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb#ch0000002vscode-remote?line=8'>9</a>\u001b[0m modelB \u001b[39m=\u001b[39m LinearRegressionClient(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/AdamHall/Documents/GitHub/datascience/notebooks/logistic_regression3.ipynb#ch0000002vscode-remote?line=9'>10</a>\u001b[0m models \u001b[39m=\u001b[39m [modelA, modelB]\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'criterion' and 'optimizer'"
     ]
    }
   ],
   "source": [
    "#Data Starts with respective owners A and B\n",
    "x_dataA = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]]))\n",
    "y_dataA = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))\n",
    "x_dataB = Variable(torch.Tensor([[7.0], [8.0], [3.0], [1.0]]))\n",
    "y_dataB = Variable(torch.Tensor([[60.0], [70.0], [40.0], [20.0]]))\n",
    "data_federation = [[x_dataA, y_dataA], [x_dataB, y_dataB]]\n",
    "        \n",
    "# modelA = LinearRegressionClient(1,1)\n",
    "# modelB = LinearRegressionClient(1,1)\n",
    "# models = [modelA, modelB]\n",
    "\n",
    "federated_model = LinearRegressionFederated(models, data_federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor([1.3727e+34])\n",
      "Loss: tensor([8.7761e+33])\n",
      "Loss: tensor([1.3438e+37])\n",
      "Loss: tensor([8.5908e+36])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n",
      "Loss: tensor([inf])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # print(federated_model.models[0].get_parameters_as_tensor())\n",
    "    # print(federated_model.models[1].get_parameters_as_tensor())\n",
    "    federated_model.train_cycle(1)\n",
    "    # print(federated_model.models[0].get_parameters_as_tensor())\n",
    "    # print(federated_model.models[1].get_parameters_as_tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a551a9fda4b0fd0844678eafd3b71ce0ebfefa198dcee60abe1926ee3f4a81d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv38_sail': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
